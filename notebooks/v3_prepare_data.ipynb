{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-08 22:48:28.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmeta_project.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\Michal\\Desktop\\MAML\\metaLearningCUB-200-2011\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_name</th>\n",
       "      <th>is_training_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>001.Black_footed_Albatross/Black_Footed_Albatr...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>001.Black_footed_Albatross</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                         image_name  image_id  class_id  \\\n",
       "0   1  001.Black_footed_Albatross/Black_Footed_Albatr...         1         1   \n",
       "1   2  001.Black_footed_Albatross/Black_Footed_Albatr...         2         1   \n",
       "2   3  001.Black_footed_Albatross/Black_Footed_Albatr...         3         1   \n",
       "3   4  001.Black_footed_Albatross/Black_Footed_Albatr...         4         1   \n",
       "4   5  001.Black_footed_Albatross/Black_Footed_Albatr...         5         1   \n",
       "\n",
       "                   class_name  is_training_image  \n",
       "0  001.Black_footed_Albatross                  0  \n",
       "1  001.Black_footed_Albatross                  1  \n",
       "2  001.Black_footed_Albatross                  0  \n",
       "3  001.Black_footed_Albatross                  1  \n",
       "4  001.Black_footed_Albatross                  1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from meta_project.data.data_loader import DataLoader\n",
    "data_loader = DataLoader()\n",
    "\n",
    "df = data_loader.load_and_merge_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data...\n",
      "Error loading data using MetaDataLoader: DataLoader.__init__() got an unexpected keyword argument 'base_path'\n",
      "Please ensure MetaDataLoader is correctly implemented and paths are set.\n",
      "Initializing model...\n",
      "Starting meta-training...\n",
      "  Batch 50/63, Approx Loss: 1.6060\n",
      "Epoch 1/100: Average Approx Loss: 1.6072\n",
      "  Batch 50/63, Approx Loss: 1.6071\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 308\u001b[39m\n\u001b[32m    305\u001b[39m epoch_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m    306\u001b[39m task_count = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmeta_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The current meta_train_batch performs the outer update inside\u001b[39;49;00m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# It returns the loss of the last task in the batch as an indicator\u001b[39;49;00m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# For a more accurate batch loss, aggregate inside meta_train_batch before backward\u001b[39;49;00m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeta_train_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Handle potential issues returning None\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 228\u001b[39m, in \u001b[36mMetaDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    227\u001b[39m     support_imgs = [\u001b[38;5;28mself\u001b[39m.transform(Image.open(os.path.join(\u001b[38;5;28mself\u001b[39m.image_dir, img)).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m support_img_names]\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     query_imgs = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m query_img_names]\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Check IMAGE_DIR and image paths.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    475\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    476\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) == \u001b[32m2\u001b[39m):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\meta-project-hyzyb4Sq-py3.12\\Lib\\site-packages\\PIL\\Image.py:2356\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2344\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2345\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2346\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2347\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2348\u001b[39m         )\n\u001b[32m   2349\u001b[39m         box = (\n\u001b[32m   2350\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2351\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2352\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2353\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from meta_project.data.data_loader import DataLoader as MetaDataLoader\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from PIL import Image\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# --- Configuration ---\n",
    "N_WAY = 5\n",
    "K_SHOT = 1\n",
    "N_QUERY = 15\n",
    "N_TASKS_PER_EPOCH = 1000\n",
    "BATCH_SIZE = 16\n",
    "LR_INNER = 0.01\n",
    "LR_OUTER = 0.001\n",
    "NUM_INNER_STEPS = 1\n",
    "IMAGE_RESIZE = 84\n",
    "IMAGE_DIR = 'data/raw/CUB_200_2011/images'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Model Definition (Adapted for MAML) ---\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_way):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2) # 84x84 -> 42x42\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2) # 42x42 -> 21x21\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2) # 21x21 -> 10x10\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(2) # 10x10 -> 5x5\n",
    "\n",
    "        flattened_size = 64 * 5 * 5\n",
    "        self.fc1 = nn.Linear(flattened_size, n_way)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# --- MAML Implementation (FOMAML using deepcopy) ---\n",
    "class MAML(nn.Module):\n",
    "    def __init__(self, model, lr_inner=0.01, lr_outer=0.001, num_inner_steps=1):\n",
    "        super(MAML, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr_inner = lr_inner\n",
    "        self.lr_outer = lr_outer\n",
    "        self.num_inner_steps = num_inner_steps\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr_outer)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.model.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.to(device))\n",
    "\n",
    "    def meta_train_batch(self, task_batch):\n",
    "        batch_x_train, batch_y_train, batch_x_test, batch_y_test = task_batch\n",
    "        num_tasks_in_batch = batch_x_train.size(0)\n",
    "\n",
    "        total_outer_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for task_idx in range(num_tasks_in_batch):\n",
    "            x_train = batch_x_train[task_idx]\n",
    "            y_train = batch_y_train[task_idx]\n",
    "            x_test = batch_x_test[task_idx]\n",
    "            y_test = batch_y_test[task_idx]\n",
    "\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "\n",
    "            # --- Inner Loop Adaptation ---\n",
    "            temp_model = copy.deepcopy(self.model)\n",
    "            temp_model.train()\n",
    "            inner_optimizer = optim.SGD(temp_model.parameters(), lr=self.lr_inner)\n",
    "\n",
    "            for _ in range(self.num_inner_steps):\n",
    "                inner_optimizer.zero_grad()\n",
    "                y_pred_support = temp_model(x_train)\n",
    "                loss_support = self.loss_fn(y_pred_support, y_train)\n",
    "                loss_support.backward()\n",
    "                inner_optimizer.step()\n",
    "\n",
    "            temp_model.eval()\n",
    "\n",
    "            y_pred_query = temp_model(x_test)\n",
    "            loss_query = self.loss_fn(y_pred_query, y_test)\n",
    "\n",
    "            total_outer_loss = total_outer_loss + loss_query\n",
    "\n",
    "        # --- Outer Loop Update ---\n",
    "        average_outer_loss = total_outer_loss / num_tasks_in_batch\n",
    "\n",
    "        average_outer_loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return average_outer_loss.item()\n",
    "\n",
    "# --- Meta Dataset ---\n",
    "class MetaDataset(Dataset):\n",
    "    def __init__(self, df, n_way, k_shot, n_query, image_dir, transform, num_tasks):\n",
    "        self.df = df\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.n_query = n_query\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "        self.classes = df['class_name'].unique().tolist()\n",
    "        self.class_to_images = {cls: df[df['class_name'] == cls]['image_name'].tolist()\n",
    "                                for cls in self.classes}\n",
    "\n",
    "        if len(self.classes) < n_way:\n",
    "            raise ValueError(f\"Dataset has only {len(self.classes)} classes, but n_way={n_way} requested.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_tasks\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sampled_classes = random.sample(self.classes, self.n_way)\n",
    "\n",
    "        task_support_images, task_support_labels = [], []\n",
    "        task_query_images, task_query_labels = [], []\n",
    "\n",
    "        task_class_map = {cls_name: i for i, cls_name in enumerate(sampled_classes)}\n",
    "\n",
    "        for cls_name in sampled_classes:\n",
    "            available_images = self.class_to_images[cls_name]\n",
    "\n",
    "            required_images = self.k_shot + self.n_query\n",
    "            if len(available_images) < required_images:\n",
    "                print(f\"Warning: Class {cls_name} has only {len(available_images)} images, sampling with replacement.\")\n",
    "                selected_indices = random.choices(range(len(available_images)), k=required_images)\n",
    "                selected_images = [available_images[i] for i in selected_indices]\n",
    "            else:\n",
    "                selected_images = random.sample(available_images, required_images)\n",
    "\n",
    "            task_label = task_class_map[cls_name]\n",
    "\n",
    "            support_img_names = selected_images[:self.k_shot]\n",
    "            query_img_names = selected_images[self.k_shot:]\n",
    "\n",
    "            try:\n",
    "                support_imgs = [self.transform(Image.open(os.path.join(self.image_dir, img)).convert('RGB')) for img in support_img_names]\n",
    "                query_imgs = [self.transform(Image.open(os.path.join(self.image_dir, img)).convert('RGB')) for img in query_img_names]\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"Error loading image: {e}. Check IMAGE_DIR and image paths.\")\n",
    "\n",
    "            task_support_images.extend(support_imgs)\n",
    "            task_support_labels.extend([task_label] * self.k_shot)\n",
    "            task_query_images.extend(query_imgs)\n",
    "            task_query_labels.extend([task_label] * self.n_query)\n",
    "\n",
    "        x_train = torch.stack(task_support_images)\n",
    "        y_train = torch.tensor(task_support_labels, dtype=torch.long)\n",
    "        x_test = torch.stack(task_query_images)\n",
    "        y_test = torch.tensor(task_query_labels, dtype=torch.long)\n",
    "\n",
    "        perm_support = torch.randperm(len(x_train))\n",
    "        x_train = x_train[perm_support]\n",
    "        y_train = y_train[perm_support]\n",
    "\n",
    "        perm_query = torch.randperm(len(x_test))\n",
    "        x_test = x_test[perm_query]\n",
    "        y_test = y_test[perm_query]\n",
    "\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "# --- Data Loading ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda image: image.convert('RGB')),\n",
    "    transforms.Resize((IMAGE_RESIZE, IMAGE_RESIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    data_loader_meta = MetaDataLoader(base_path='.')\n",
    "    df = data_loader_meta.load_and_merge_data()\n",
    "    print(f\"Loaded DataFrame with {len(df)} entries.\")\n",
    "    print(f\"Image directory used: {os.path.abspath(IMAGE_DIR)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data using MetaDataLoader: {e}\")\n",
    "    print(\"Please ensure MetaDataLoader is correctly implemented and paths are set.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "meta_dataset = MetaDataset(df, n_way=N_WAY, k_shot=K_SHOT, n_query=N_QUERY,\n",
    "                           image_dir=IMAGE_DIR, transform=transform, num_tasks=N_TASKS_PER_EPOCH)\n",
    "\n",
    "meta_dataloader = DataLoader(meta_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "# --- Initialization and Training Loop ---\n",
    "print(\"Initializing model...\")\n",
    "model = SimpleCNN(n_way=N_WAY)\n",
    "maml = MAML(model, lr_inner=LR_INNER, lr_outer=LR_OUTER, num_inner_steps=NUM_INNER_STEPS)\n",
    "\n",
    "print(\"Starting meta-training...\")\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    maml.model.train()\n",
    "    epoch_loss = 0.0\n",
    "    task_count = 0\n",
    "\n",
    "    for i, task_batch in enumerate(meta_dataloader):\n",
    "        loss = maml.meta_train_batch(task_batch)\n",
    "\n",
    "        if loss is not None:\n",
    "            epoch_loss += loss\n",
    "            task_count += len(task_batch)\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "             print(f\"  Batch {i+1}/{len(meta_dataloader)}, Approx Loss: {loss:.4f}\")\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / (i+1)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Average Approx Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Meta-training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
